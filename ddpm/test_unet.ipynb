{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ddpm.data import CustomMnistDataset\n",
    "from ddpm.config import CONFIG\n",
    "from ddpm.unet_utils import Unet\n",
    "from ddpm.process import ForwardProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading MNIST dataset...\")\ntrain_csv = CONFIG.data_dir / \"train.csv\"\nmnist_ds = CustomMnistDataset(str(train_csv))\nmnist_dl = DataLoader(mnist_ds, batch_size=128, shuffle=False)\n\nprint(f\"Dataset size: {len(mnist_ds)} images\")\nprint(f\"Number of batches: {len(mnist_dl)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating mean and std...\\n\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for batch in tqdm(mnist_dl, desc=\"Loading batches\"):\n",
    "    all_data.append(batch)\n",
    "\n",
    "all_data = torch.cat(all_data, dim=0)\n",
    "print(f\"Total data shape: {all_data.shape}\\n\")\n",
    "\n",
    "mean = torch.mean(all_data).item()\n",
    "std = torch.std(all_data).item()\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"MNIST Statistics (after standardization to [-1, 1]):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Mean: {mean:.6f}\")\n",
    "print(f\"Std:  {std:.6f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"\\nData min value: {all_data.min().item():.4f}\")\n",
    "print(f\"Data max value: {all_data.max().item():.4f}\")\n",
    "\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "model = Unet().to(device)\n",
    "print(\"Created untrained UNet with random weights\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {num_params:,}\\n\")\n",
    "\n",
    "fp = ForwardProcess()\n",
    "\n",
    "fp.betas = fp.betas.to(device)\n",
    "fp.sqrt_betas = fp.sqrt_betas.to(device)\n",
    "fp.alphas = fp.alphas.to(device)\n",
    "fp.sqrt_alphas = fp.sqrt_alphas.to(device)\n",
    "fp.alpha_bars = fp.alpha_bars.to(device)\n",
    "fp.sqrt_alpha_bars = fp.sqrt_alpha_bars.to(device)\n",
    "fp.sqrt_one_minus_alpha_bars = fp.sqrt_one_minus_alpha_bars.to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(mnist_dl)).to(device)\n",
    "print(f\"Batch shape: {imgs.shape}\")\n",
    "print(f\"Batch size: {imgs.shape[0]}\\n\")\n",
    "\n",
    "noise = torch.randn_like(imgs).to(device)\n",
    "t = torch.randint(0, CONFIG.num_timesteps, (imgs.shape[0],)).to(device)\n",
    "noisy_imgs = fp.add_noise(imgs, noise, t)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    noise_pred = model(noisy_imgs, t)\n",
    "    loss = criterion(noise_pred, noise)\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Initial Loss (Untrained UNet):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"MSE Loss: {loss.item():.6f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmonrqmpwq",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Setting up training for gradient flow check\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "mnist_dl_train = DataLoader(mnist_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "model_train = Unet().to(device)\n",
    "optimizer = torch.optim.Adam(model_train.parameters(), lr=1e-4)\n",
    "model_train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vicw7aopfif",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Training for 3 epochs to monitor gradients\\n\")\n",
    "\n",
    "# Store stats across all epochs\n",
    "all_epoch_losses = []\n",
    "all_mean_abs_grads = []\n",
    "all_max_abs_grads = []\n",
    "all_grad_norms = []\n",
    "all_zero_grad_pcts = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    epoch_losses = []\n",
    "    epoch_grad_stats = {\n",
    "        \"mean_abs_grad\": [],\n",
    "        \"max_abs_grad\": [],\n",
    "        \"grad_norm\": [],\n",
    "        \"zero_grad_percentage\": []\n",
    "    }\n",
    "    \n",
    "    for imgs in tqdm(mnist_dl_train, desc=f\"Epoch {epoch+1}/3\"):\n",
    "        imgs = imgs.to(device)\n",
    "        noise = torch.randn_like(imgs).to(device)\n",
    "        t = torch.randint(0, CONFIG.num_timesteps, (imgs.shape[0],)).to(device)\n",
    "        noisy_imgs = fp.add_noise(imgs, noise, t)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        noise_pred = model_train(noisy_imgs, t)\n",
    "        loss = criterion(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        \n",
    "        all_grads = []\n",
    "        for param in model_train.parameters():\n",
    "            if param.grad is not None:\n",
    "                all_grads.append(param.grad.flatten())\n",
    "        \n",
    "        if len(all_grads) > 0:\n",
    "            all_grads = torch.cat(all_grads)\n",
    "            \n",
    "            mean_abs_grad = torch.mean(torch.abs(all_grads)).item()\n",
    "            max_abs_grad = torch.max(torch.abs(all_grads)).item()\n",
    "            grad_norm = torch.norm(all_grads).item()\n",
    "            zero_grad_pct = (torch.sum(torch.abs(all_grads) < 1e-7).item() / all_grads.numel()) * 100\n",
    "            \n",
    "            epoch_grad_stats[\"mean_abs_grad\"].append(mean_abs_grad)\n",
    "            epoch_grad_stats[\"max_abs_grad\"].append(max_abs_grad)\n",
    "            epoch_grad_stats[\"grad_norm\"].append(grad_norm)\n",
    "            epoch_grad_stats[\"zero_grad_percentage\"].append(zero_grad_pct)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    # Store epoch averages\n",
    "    all_epoch_losses.append(np.mean(epoch_losses))\n",
    "    all_mean_abs_grads.append(np.mean(epoch_grad_stats[\"mean_abs_grad\"]))\n",
    "    all_max_abs_grads.append(np.mean(epoch_grad_stats[\"max_abs_grad\"]))\n",
    "    all_grad_norms.append(np.mean(epoch_grad_stats[\"grad_norm\"]))\n",
    "    all_zero_grad_pcts.append(np.mean(epoch_grad_stats[\"zero_grad_percentage\"]))\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}: Loss={np.mean(epoch_losses):.6f}\")\n",
    "\n",
    "# Plot gradient statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "epochs = range(1, 4)\n",
    "\n",
    "# Loss plot\n",
    "axes[0, 0].plot(epochs, all_epoch_losses, 'b-o', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean absolute gradient\n",
    "axes[0, 1].plot(epochs, all_mean_abs_grads, 'g-o', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Mean |grad|', fontsize=12)\n",
    "axes[0, 1].set_title('Mean Absolute Gradient', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# Gradient norm\n",
    "axes[1, 0].plot(epochs, all_grad_norms, 'r-o', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Gradient Norm', fontsize=12)\n",
    "axes[1, 0].set_title('Gradient Norm', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# Zero gradient percentage\n",
    "axes[1, 1].plot(epochs, all_zero_grad_pcts, 'm-o', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Zero Grad %', fontsize=12)\n",
    "axes[1, 1].set_title('Zero Gradient Percentage', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Final Loss: {all_epoch_losses[-1]:.6f}\")\n",
    "print(f\"Final Mean |grad|: {all_mean_abs_grads[-1]:.6e}\")\n",
    "print(f\"Final Grad Norm: {all_grad_norms[-1]:.6e}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44wihlsra82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Forward Process Visualization\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "sample_img = mnist_ds[0].unsqueeze(0).to(device)\n",
    "\n",
    "timesteps = [1, 2, 5, 10, 20, 50, 100, 500, 999]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(sample_img[0, 0].cpu().numpy(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "axes[0].set_title(\"Original (t=0)\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "for i, t_val in enumerate(timesteps):\n",
    "    noise = torch.randn_like(sample_img)\n",
    "    t_tensor = torch.tensor([t_val], device=device)\n",
    "    noisy_img = fp.add_noise(sample_img, noise, t_tensor)\n",
    "    \n",
    "    axes[i+1].imshow(noisy_img[0, 0].cpu().numpy(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    axes[i+1].set_title(f\"t={t_val}\")\n",
    "    axes[i+1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}